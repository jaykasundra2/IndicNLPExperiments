{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd86d2d534c84dab90e13a540741be91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e59211591eb48e69ddad0d51988257f",
              "IPY_MODEL_7dc9e4f0c2854eeea61cca21065334ce",
              "IPY_MODEL_a4d5d5adb6f54f138f1fe689551b1eec"
            ],
            "layout": "IPY_MODEL_089b883d24d64fc29b5784baba57af58"
          }
        },
        "6e59211591eb48e69ddad0d51988257f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_603227fe2247432c83b8a2cfce7219f9",
            "placeholder": "​",
            "style": "IPY_MODEL_ec13de81126241bf98f84aa51b024740",
            "value": "Downloading readme: 100%"
          }
        },
        "7dc9e4f0c2854eeea61cca21065334ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7b7aa74a21244db82ef16e2e511d2c8",
            "max": 8395,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_011dca442ec743ae8c0eef07815bec17",
            "value": 8395
          }
        },
        "a4d5d5adb6f54f138f1fe689551b1eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef5cd6b3a9ed48d8b94588fc959212c7",
            "placeholder": "​",
            "style": "IPY_MODEL_cb424b9dfe2b4ae883552e22458fabed",
            "value": " 8.39k/8.39k [00:00&lt;00:00, 291kB/s]"
          }
        },
        "089b883d24d64fc29b5784baba57af58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "603227fe2247432c83b8a2cfce7219f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec13de81126241bf98f84aa51b024740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7b7aa74a21244db82ef16e2e511d2c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "011dca442ec743ae8c0eef07815bec17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef5cd6b3a9ed48d8b94588fc959212c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb424b9dfe2b4ae883552e22458fabed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrAL5wuk6Fp4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/anoopkunchukuttan/crowd-indic-transliteration-data/blob/master/crowd_transliterations.hi-en.txt\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ai4bharat/Aksharantar\",split='train',data_files=['hin.zip'], streaming=True)\n",
        "ds = list(dataset.take(5000))\n",
        "# ds = list(dataset)\n",
        "ds[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "bd86d2d534c84dab90e13a540741be91",
            "6e59211591eb48e69ddad0d51988257f",
            "7dc9e4f0c2854eeea61cca21065334ce",
            "a4d5d5adb6f54f138f1fe689551b1eec",
            "089b883d24d64fc29b5784baba57af58",
            "603227fe2247432c83b8a2cfce7219f9",
            "ec13de81126241bf98f84aa51b024740",
            "f7b7aa74a21244db82ef16e2e511d2c8",
            "011dca442ec743ae8c0eef07815bec17",
            "ef5cd6b3a9ed48d8b94588fc959212c7",
            "cb424b9dfe2b4ae883552e22458fabed"
          ]
        },
        "id": "-mX_5h-K6GjS",
        "outputId": "6e434f65-5be2-4944-e085-e21c9b899678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.39k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd86d2d534c84dab90e13a540741be91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration ai4bharat--Aksharantar-83c643ad2c9e6ab1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'unique_identifier': 'hin1',\n",
              "  'native word': 'मैट्रोलॉजिस्ट',\n",
              "  'english word': 'maitrologist',\n",
              "  'source': 'AK-Freq'},\n",
              " {'unique_identifier': 'hin2',\n",
              "  'native word': 'पीएचडब्ल्यूसीएस',\n",
              "  'english word': 'phwcs',\n",
              "  'source': 'AK-Freq'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSuC8vqUzL1W",
        "outputId": "05e46108-82a3-4ee5-d662-45d073f74405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hin_words = [i['native word'] for i in ds]\n",
        "hin_vocab = set(list(\"\".join(hin_words)))\n",
        "print(len(list(hin_vocab)))\n",
        "eng_words = [i['english word'] for i in ds]\n",
        "eng_vocab = set(list(\"\".join(eng_words)))\n",
        "print(len(list(eng_vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08gZ6jbJ6Xbv",
        "outputId": "76936040-dc9f-4af8-bc27-6e19a567b849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX , BOS_IDX, EOS_IDX = 0, 1, 2\n",
        "num_of_special_tokens = 3\n",
        "stoi_input = {}\n",
        "stoi_input['#'] = PAD_IDX\n",
        "stoi_input['@'] = BOS_IDX\n",
        "stoi_input['$'] = EOS_IDX\n",
        "stoi_input.update({s:i+num_of_special_tokens for i,s in enumerate(hin_vocab)})\n",
        "itos_input = {v:k for k,v in stoi_input.items()}\n",
        "\n",
        "stoi_output = {}\n",
        "stoi_output['#'] = PAD_IDX\n",
        "stoi_output['@'] = BOS_IDX\n",
        "stoi_output['$'] = EOS_IDX\n",
        "stoi_output.update({s:i+num_of_special_tokens for i,s in enumerate(eng_vocab)})\n",
        "itos_output = {v:k for k,v in stoi_output.items()}"
      ],
      "metadata": {
        "id": "I8qK2fuw_jG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stoi_input),len(itos_input),len(stoi_output), len(itos_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViquzCI3NzQ1",
        "outputId": "21acd845-8f23-4ab3-95ab-25590e9ecab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67, 67, 29, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input(txt):\n",
        "  out = []\n",
        "  for ch in txt:\n",
        "    out.append(stoi_input[ch])\n",
        "  return out\n",
        "def decode_input(ids):\n",
        "  out = []\n",
        "  for id in ids:\n",
        "    out.append(itos_input[id])\n",
        "  return \"\".join(out)\n",
        "def encode_output(txt):\n",
        "  out = []\n",
        "  for ch in txt:\n",
        "    out.append(stoi_output[ch])\n",
        "  return out\n",
        "def decode_output(ids):\n",
        "  out = []\n",
        "  for id in ids:\n",
        "    out.append(itos_output[id])\n",
        "  return \"\".join(out)\n",
        "encoded_input_ids = encode_input(\"मैट्रोलॉजिस्ट\")\n",
        "print(encoded_input_ids)\n",
        "decoded_input = decode_input(encoded_input_ids)\n",
        "print(decoded_input)\n",
        "encoded_output_ids = encode_output(\"maitrologist\")\n",
        "print(encoded_output_ids)\n",
        "decoded_output = decode_output(encoded_output_ids)\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lam7CbHkA0JZ",
        "outputId": "88b21de6-ce0c-4527-ca86-de195c1f80bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[66, 58, 34, 38, 32, 63, 59, 62, 23, 7, 39, 38, 34]\n",
            "मैट्रोलॉजिस्ट\n",
            "[21, 3, 18, 12, 17, 15, 7, 15, 23, 18, 22, 12]\n",
            "maitrologist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = [encode_input(\"@\" + i['native word'] + \"$\") for i in ds]\n",
        "# y = [encode_output(\"@\" + i['english word'] + \"$\") for i in ds]\n",
        "# x[0], y[0], decode_input(x[0]), decode_output(y[0])"
      ],
      "metadata": {
        "id": "L1ZICoWcEd3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Custom Dataset class\n",
        "class TextToTextDataset(Dataset):\n",
        "    def __init__(self, input_data, target_data):\n",
        "        self.input_data = input_data\n",
        "        self.target_data = target_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_word = self.input_data[idx]\n",
        "        target_word = self.target_data[idx]\n",
        "\n",
        "        input_indices = torch.tensor(encode_input(input_word))\n",
        "        target_indices = torch.tensor(encode_output(target_word))\n",
        "\n",
        "        return input_indices, target_indices\n",
        "\n",
        "# Pad sequences in a batch\n",
        "def collate_fn(batch):\n",
        "    input_seqs, target_seqs = zip(*batch)\n",
        "    # Pad input sequences\n",
        "    input_lengths = torch.tensor([len(seq) for seq in input_seqs])\n",
        "    max_input_length = max(input_lengths)\n",
        "    padded_input_seqs = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad target sequences\n",
        "    target_lengths = torch.tensor([len(seq) for seq in target_seqs])\n",
        "    max_target_length = max(target_lengths)\n",
        "    padded_target_seqs = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
        "    # target_seqs = torch.stack(target_seqs,dim=0)\n",
        "\n",
        "    return padded_input_seqs, padded_target_seqs\n",
        "\n",
        "# Create train, validation, and test datasets\n",
        "train_input_data = [\"@\"+i['native word']+\"$\" for i in ds]\n",
        "train_target_data = [\"@\"+i['english word']+\"$\" for i in ds]\n",
        "\n",
        "# Create train, validation, and test datasets\n",
        "train_dataset = TextToTextDataset(train_input_data, train_target_data)\n",
        "# valid_dataset = TextToTextDataset(valid_input_data, train_target_data)\n",
        "# test_dataset = TextToTextDataset(test_input_data, train_target_data)\n",
        "\n",
        "# Create DataLoader for each dataset with padding\n",
        "batch_size = 256\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "# valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "0dau2BjqIFIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_input, batch_target in train_dataloader:\n",
        "  for i in range(batch_size):\n",
        "    print(decode_input(batch_input[i].tolist()), decode_output(batch_target[i].tolist()))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_omPT_J-JIB9",
        "outputId": "c535a7e6-a3be-43ee-8cda-e757ccf4e45a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@लॉवेल$############## @lowell$#################\n",
            "@किंगखान$############ @kingkhan$###############\n",
            "@गौर$################ @gaur$###################\n",
            "@सुरभि$############## @surabhi$################\n",
            "@ब्रह्मचर्चाओं$###### @bhramacharchaon$########\n",
            "@दमघोंटू$############ @damghontu$##############\n",
            "@सॉलिस$############## @solis$##################\n",
            "@रोज़ा$############## @rosa$###################\n",
            "@एक्सिसटेंस$######### @existence$##############\n",
            "@प्रदेशवाशियों$###### @pradeshvashiyon$########\n",
            "@प्राना$############# @prana$##################\n",
            "@नईदिल्ली$########### @naeedillee$#############\n",
            "@याचिकाकर्ता$######## @yachikakarta$###########\n",
            "@सिरती$############## @sirtee$#################\n",
            "@नरसीपुरा$########### @naraseepura$############\n",
            "@मैक्सियों$########## @maxiyon$################\n",
            "@नागमण्डल$########### @nagmandal$##############\n",
            "@रैलीः$############## @railih$#################\n",
            "@हामिदन$############# @hamidan$################\n",
            "@बज$################# @budge$##################\n",
            "@ऑक्टा$############## @octa$###################\n",
            "@दुष्टतापूर्वक$###### @dushtapurvak$###########\n",
            "@गेवारा$############# @guevara$################\n",
            "@मैनवल$############## @mainval$################\n",
            "@धनश्याम$############ @dhanashyaam$############\n",
            "@जनशिक्षा$########### @janashiksha$############\n",
            "@भीगने$############## @bheegane$###############\n",
            "@ऐजेंसियों$########## @agencyyon$##############\n",
            "@लिंमर्ग$############ @linmarga$###############\n",
            "@बिच्चों$############ @bichchon$###############\n",
            "@लिखेहैं$############ @likhehain$##############\n",
            "@अनट्रेंड$########### @untrend$################\n",
            "@क्यूडब्ल्यूवीजीए$### @qwvga$##################\n",
            "@पुनर्विज्ञापन$###### @punarvigyaapan$#########\n",
            "@बहुरिया$############ @behuria$################\n",
            "@एचजेडटीसी$########## @echajedateesee$#########\n",
            "@एनसीआरडब्ल्यूसी$#### @ncrwc$##################\n",
            "@सीमाई$############## @seemaai$################\n",
            "@अरेना$############## @arena$##################\n",
            "@खिताबों$############ @khitaabon$##############\n",
            "@एनरिकेज़$########### @enriquez$###############\n",
            "@अनाधिकारिक$######### @anaadhikaarik$##########\n",
            "@क्वॉरेनटाइन$######## @kworentyn$##############\n",
            "@एस्वेडो$############ @acevedo$################\n",
            "@चेरी$############### @cherry$#################\n",
            "@आत्मविश्लेषक$####### @aatmawishleshak$########\n",
            "@व्यवस्थाविरोधी$##### @vyavasthavirodhi$#######\n",
            "@सोसियेदाद$########## @sosiyedaad$#############\n",
            "@प्रत्याक्षीयों$##### @pratyaaksheeyon$########\n",
            "@उम्मीदवारी$######### @ummidwaaree$############\n",
            "@चिमाते$############# @chimaate$###############\n",
            "@परिवारः$############ @parivaarah$#############\n",
            "@स्ट्रॉन्ग$########## @strong$#################\n",
            "@सूत्रपाडा$########## @sutrapadaa$#############\n",
            "@मोंटे$############## @monte$##################\n",
            "@राखेचा$############# @rakhecha$###############\n",
            "@बंटता$############## @bantataa$###############\n",
            "@लूई$################ @louis$##################\n",
            "@याद्दाश्त$########## @yaddasht$###############\n",
            "@प्राणसंगली$######### @pransangalee$###########\n",
            "@फ़्यूनटेस$########## @fuentes$################\n",
            "@प्राणसंगली$######### @pransangali$############\n",
            "@सुंदरम$############# @sundram$################\n",
            "@क़रार$############## @karaar$#################\n",
            "@गुना$############### @guna$###################\n",
            "@छोडिये$############# @chhodiye$###############\n",
            "@पूर्वाेत्तर$######## @poorwottar$#############\n",
            "@तदंरुस्ती$########## @tadanrusti$#############\n",
            "@शिवालापुरवा$######## @shivaalapurva$##########\n",
            "@व्याख्यात्मकता$##### @vyaakhyaatmaktaa$#######\n",
            "@मंगलमूर्तिः$######## @mangalmurtih$###########\n",
            "@जिलावासियों$######## @jilawasiyon$############\n",
            "@खनौदा$############## @khanaudaa$##############\n",
            "@प्रस्थानों$######### @prasthanon$#############\n",
            "@तेलुगुदेशम$######### @telugudesham$###########\n",
            "@चिकित्साएवं$######## @chikitsaevam$###########\n",
            "@आईएफसीआई$########### @ifci$###################\n",
            "@प्रसारकों$########## @prasarakon$#############\n",
            "@जोंस$############### @jones$##################\n",
            "@हरमन$############### @herman$#################\n",
            "@कुक$################ @cook$###################\n",
            "@तुलसीदास$########### @tulshidas$##############\n",
            "@मुक्तिमें$########## @muktimen$###############\n",
            "@रामनामी$############ @raamanaamee$############\n",
            "@राजनीचि$############ @rajneechi$##############\n",
            "@नोटबंदीशुदा$######## @notbandeeshuda$#########\n",
            "@माथुर$############## @mathur$#################\n",
            "@जवन$################ @javan$##################\n",
            "@चन्द्रदास$########## @chandradas$#############\n",
            "@ब्रह्राचारिणी$###### @brahracharini$##########\n",
            "@वर्स्टेड$########### @warsted$################\n",
            "@अलगअलग$############# @alagalag$###############\n",
            "@सहिंता$############# @sahintaa$###############\n",
            "@बडयाल$############## @badyal$#################\n",
            "@इनसानी$############# @insaani$################\n",
            "@बर्न्सविले$######### @burnsville$#############\n",
            "@रिश्वतखोर$########## @rishvatkhor$############\n",
            "@पत्रकारवार्ता$###### @patrakaarwarta$#########\n",
            "@उद्घाटनों$########## @udghaatanon$############\n",
            "@जाऐगा$############## @jayegaa$################\n",
            "@हाथे$############### @haathe$#################\n",
            "@गोंडा$############## @gonda$##################\n",
            "@उबाला$############## @ubalaa$#################\n",
            "@मनोकामनाएं$######### @manokaamnaaen$##########\n",
            "@नगरायुक्त$########## @nagarayukta$############\n",
            "@मायो$############### @mayo$###################\n",
            "@मंत्रयों$########### @mantrayon$##############\n",
            "@राजनीतिकारों$####### @rajneetikaaron$#########\n",
            "@समटेर$############## @sumter$#################\n",
            "@अखुआपाड़ा$########## @akhuaapadaa$############\n",
            "@भावाभिव्यक्तियों$### @bhavabhivyaktiyon$######\n",
            "@निखरेगा$############ @nikharegaa$#############\n",
            "@रोसेल्स$############ @rosales$################\n",
            "@घटबढ़$############## @ghatabadh$##############\n",
            "@कार्यकार्यकर्ताओं$## @karyakaaryakartaon$#####\n",
            "@क्लेन$############## @klein$##################\n",
            "@महेंद्रभटनागर$###### @mahendrabhatanaagar$####\n",
            "@भरष्टाचारियों$###### @bharashtachariyon$######\n",
            "@परीक्षणकर्ताओं$##### @parikshankartaaon$######\n",
            "@टॉकशो$############## @talkshow$###############\n",
            "@हेस$################ @hess$###################\n",
            "@मौजुद$############## @moujud$#################\n",
            "@चड़ी$############### @chadhee$################\n",
            "@मैसी$############### @massey$#################\n",
            "@बार्कर$############# @barker$#################\n",
            "@ऐसो$################ @aiso$###################\n",
            "@अनिष्टता$########### @anishtataa$#############\n",
            "@वुल्फ$############## @wolf$###################\n",
            "@यूरोप्लास्ट$######## @uroplast$###############\n",
            "@मंत्रीपद$########### @mantreepad$#############\n",
            "@बैनामा$############# @bainaama$###############\n",
            "@सॉन्डर्स$########### @saunders$###############\n",
            "@उठिए$############### @uthie$##################\n",
            "@माउंटेन$############ @mountain$###############\n",
            "@स्पिन्ज$############ @spinj$##################\n",
            "@धातु$############### @dhatu$##################\n",
            "@अर्थशास्त्रीयों$#### @arthshaastreeyon$#######\n",
            "@जियाऊर$############# @jiyaaoor$###############\n",
            "@प्रूफ़रीड$########## @proofreed$##############\n",
            "@बर्बरतापूर्ण$####### @barbartapoorna$#########\n",
            "@ब्रिगेड$############ @brigade$################\n",
            "@ज्यादातियों$######## @jyadatiyon$#############\n",
            "@रेजिस्टेस$########## @registes$###############\n",
            "@मुरफ्रीसबोरो$####### @murfreesboro$###########\n",
            "@उंई$################ @unee$###################\n",
            "@बुद्धिमानों$######## @buddhimaanon$###########\n",
            "@जयकारा$############# @jaykara$################\n",
            "@साबुदाना$########### @saabudaanaa$############\n",
            "@काठियावड़ी$######### @kathiyawadi$############\n",
            "@कोचेला$############# @coachella$##############\n",
            "@सेंटेंनियल$######### @centennial$#############\n",
            "@लैच$################ @leach$##################\n",
            "@मिथ्याचारियों$###### @mithyaachaariyon$#######\n",
            "@वेरनन$############## @vernon$#################\n",
            "@रूकेंगे$############ @rookenge$###############\n",
            "@सीताराम$############ @sitaram$################\n",
            "@पिट्यूनिया$######### @pityooniyaa$############\n",
            "@पूर्वापेक्षाओं$##### @purvapekshaon$##########\n",
            "@द्विरुक्त$########## @dwirukta$###############\n",
            "@विधि$############### @vidhi$##################\n",
            "@स्पेनिश$############ @spanish$################\n",
            "@दिव्यतापूर्ण$####### @diwyataapoorn$##########\n",
            "@अनोंदिता$########### @anondita$###############\n",
            "@निष्कर्षः$########## @nishkarshah$############\n",
            "@कैंब्रिज$########### @cambridge$##############\n",
            "@बरहड़वाः$########### @barahadhawaah$##########\n",
            "@मायाजाल$############ @maayaajaal$#############\n",
            "@एनडब्ल्यूसी$######## @nwc$####################\n",
            "@बुद्धाराम$########## @buddharaam$#############\n",
            "@रोवे$############### @rowe$###################\n",
            "@मलमास$############## @malmaas$################\n",
            "@विज्ञानवादियों$##### @vigyanwaadiyon$#########\n",
            "@इनसानी$############# @insani$#################\n",
            "@पार्कर$############# @parker$#################\n",
            "@अंतःक्षेत्रों$###### @antahkshetron$##########\n",
            "@क्यूसैक्स$########## @kyusaiks$###############\n",
            "@वर्णव्यवस्थावादियों$ @warnavyavasthaavaadiyon$\n",
            "@सेवड$############### @sewad$##################\n",
            "@दुबे$############### @dubey$##################\n",
            "@क्रेमर$############# @kramer$#################\n",
            "@हिंया$############## @hinya$##################\n",
            "@ब्यूटीशियंस$######## @byooteeshiyans$#########\n",
            "@नारियलयुक्त$######## @naariyalyukt$###########\n",
            "@कार्यकार्यकर्ताओं$## @kaaryakaryakartaon$#####\n",
            "@विश्ववीर$########### @wishwaweer$#############\n",
            "@विश्वासघातकों$###### @vishwasghatakon$########\n",
            "@परिवारवाले$######### @parivaarwaale$##########\n",
            "@स्वायत्तशासन$####### @svayattshaasan$#########\n",
            "@माईकल$############## @michael$################\n",
            "@मिसौला$############# @missoula$###############\n",
            "@डिअरबॉर्न$########## @dearborn$###############\n",
            "@एनएमडीसी$########### @nmdc$###################\n",
            "@दृष्टिहीन$########## @drishtiheen$############\n",
            "@कासनिया$############ @kasniya$################\n",
            "@स्वानसन$############ @swanson$################\n",
            "@विश्ववंद्या$######## @vishvwandhyaa$##########\n",
            "@विष्णुपुरा$######### @vishnupura$#############\n",
            "@पैंसठ$############## @paisath$################\n",
            "@हत्यारोपित$######### @hatyaaropit$############\n",
            "@वेब$################ @webb$###################\n",
            "@महासेठ$############# @mahaaseth$##############\n",
            "@उतरियो$############# @utariyo$################\n",
            "@देशभक्तिमय$######### @deshbhaktimaya$#########\n",
            "@ऑलआउट$############## @allout$#################\n",
            "@मंत्रिमंडलीय$####### @mantrimandleey$#########\n",
            "@उखड़े$############## @ukhade$#################\n",
            "@मलौन$############### @malaun$#################\n",
            "@चन्द्रदास$########## @chandradaas$############\n",
            "@कलबुरगी$############ @kalaburagi$#############\n",
            "@चुक्र$############## @chukra$#################\n",
            "@सासाराम$############ @sasaram$################\n",
            "@कंटिन्यू$########### @continue$###############\n",
            "@गाड़ूँगा$########### @gaadhoongaa$############\n",
            "@अंदरखाने$########### @andarkhane$#############\n",
            "@एक्सक्लेशन$######### @acsclation$#############\n",
            "@भटकाना$############# @bhatkanaa$##############\n",
            "@सैनपुर$############# @sainpur$################\n",
            "@लुभाते$############# @lubhate$################\n",
            "@चूचियां$############ @choochiyan$#############\n",
            "@पिथौरागढ़$########## @pithoragarh$############\n",
            "@अनुमार$############# @anumar$#################\n",
            "@फाइब्रिक$########### @faibric$################\n",
            "@एसटेस$############## @estes$##################\n",
            "@ब्लांकार्ड$######### @blankaard$##############\n",
            "@हरिद्वार$########### @haridwar$###############\n",
            "@चटकाते$############# @chatakaate$#############\n",
            "@पुर्नस्वीकृत$####### @purnsvikrit$############\n",
            "@प्रदेशवाशियों$###### @pradeshwashiyon$########\n",
            "@एस्टेट्स$########### @estates$################\n",
            "@मान्यताओ$########### @maanyataao$#############\n",
            "@सतलज$############### @sutlej$#################\n",
            "@मांगीना$############ @maangina$###############\n",
            "@डीविलियर्स$######### @diviliyars$#############\n",
            "@निपटाकर$############ @niptakar$###############\n",
            "@मंगलसिंह$########### @mangalsinh$#############\n",
            "@केस्को$############# @kesko$##################\n",
            "@अन्त्यन्त$########## @antayant$###############\n",
            "@एनिड$############### @enid$###################\n",
            "@स्टीन$############## @stein$##################\n",
            "@कदमो$############### @kadmo$##################\n",
            "@नौंवीं$############# @naunveen$###############\n",
            "@डिडौली$############# @didaulee$###############\n",
            "@एक्सिसटेंस$######### @eksisatens$#############\n",
            "@जुटाती$############# @jutati$#################\n",
            "@दमघोंटू$############ @dumghontu$##############\n",
            "@अट्रैक्ट$########### @atraikt$################\n",
            "@मैकलॉक्लिन$######### @mclaughlin$#############\n",
            "@पीडब्ल्यूआईडी$###### @peedablyooaaidee$#######\n",
            "@चच्चा$############## @chacchaa$###############\n",
            "@बस्तियो$############ @bastiyo$################\n",
            "@प्रतिष्ठावाले$###### @pratishthavale$#########\n",
            "@मार्क्स$############ @marks$##################\n",
            "@जगदीशचंद्र$######### @jagadishachandra$#######\n",
            "@हिचकिचा$############ @hichkicha$##############\n",
            "@उठनी$############### @uthani$#################\n",
            "@चंद्रेश्वरनगर$###### @chandreshwaranagar$#####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import math\n",
        "\n",
        "\n",
        "# class PositionalEncoding(nn.Module):\n",
        "#     def __init__(self, d_model, max_len=1000):\n",
        "#         super(PositionalEncoding, self).__init__()\n",
        "#         self.dropout = nn.Dropout(p=0.1)\n",
        "#         pe = torch.zeros(max_len, d_model)\n",
        "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "#         self.register_buffer('pe', pe)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.pe[:x.size(0), :]\n",
        "#         return self.dropout(x)\n",
        "\n",
        "# class TransformerTextToText(nn.Module):\n",
        "#     def __init__(self, input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
        "#         super(TransformerTextToText, self).__init__()\n",
        "#         self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "#         self.positional_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "#         self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "#         self.decoder_embedding = nn.Embedding(output_vocab_size, d_model)\n",
        "#         self.decoder_positional_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "#         self.fc = nn.Linear(d_model, output_vocab_size)\n",
        "\n",
        "#     def forward(self, src, tgt):\n",
        "#         src = self.embedding(src)\n",
        "#         src = self.positional_encoding(src)\n",
        "\n",
        "#         tgt = self.decoder_embedding(tgt)\n",
        "#         tgt = self.decoder_positional_encoding(tgt)\n",
        "\n",
        "#         output = self.transformer(src, tgt)\n",
        "#         output = self.fc(output)\n",
        "\n",
        "#         return output"
      ],
      "metadata": {
        "id": "LQ0xGkvNcMte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# # Define hyperparameters\n",
        "# input_vocab_size = len(stoi_input)\n",
        "# output_vocab_size = len(stoi_output)\n",
        "# d_model = 32\n",
        "# nhead = 4\n",
        "# num_encoder_layers = 1\n",
        "# num_decoder_layers = 1\n",
        "# model = TransformerTextToText(input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# num_epochs = 1\n",
        "\n",
        "# # Inside the training loop\n",
        "# def train_batch(input_tensor, target_tensor):\n",
        "#     optimizer.zero_grad()\n",
        "\n",
        "#     # Generate mask for the padded tokens\n",
        "#     src_key_padding_mask = (input_tensor == 63)  # Assuming 63 is the index of the padding token\n",
        "\n",
        "#     # Apply padding mask to the input_tensor\n",
        "#     input_tensor = input_tensor.masked_fill(src_key_padding_mask, 0)\n",
        "\n",
        "#     # Generate mask for the padded tokens in the decoder\n",
        "#     tgt_key_padding_mask = (target_tensor == 26)  # Assuming 26 is the index of the padding token\n",
        "\n",
        "#     # Forward pass\n",
        "#     output = model(input_tensor, target_tensor, tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "\n",
        "#     output = output.view(-1, output_vocab_size)\n",
        "#     target_tensor = target_tensor.view(-1)\n",
        "#     loss = criterion(output, target_tensor)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# # Example training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch_input, batch_target in train_dataloader:\n",
        "#         batch_input = batch_input.to(device)\n",
        "#         batch_target = batch_target.to(device)\n",
        "\n",
        "#         loss = train_batch(batch_input, batch_target)\n",
        "#         total_loss += loss\n",
        "\n",
        "#     avg_loss = total_loss / len(train_dataloader)\n",
        "#     print(f\"Epoch [{epoch+1}/{num_epochs}] - Avg. Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "frrY3oqfL7ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(stoi_input)\n",
        "TGT_VOCAB_SIZE = len(stoi_output)\n",
        "EMB_SIZE = 128\n",
        "NHEAD = 4\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 1\n",
        "NUM_DECODER_LAYERS = 1\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "      src_batch.append(torch.tensor(encode_input(src_sample)))\n",
        "      tgt_batch.append(torch.tensor(encode_output(tgt_sample)))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch.T, tgt_batch.T\n",
        "# batch = [(i, j) for i, j in zip(train_input_data[:10], train_target_data[:10])]\n",
        "# batch\n",
        "# collate_fn(batch)\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, input_data, target_data):\n",
        "        self.input_data = input_data\n",
        "        self.target_data = target_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_data[idx], self.target_data[idx]\n",
        "\n",
        "train_dataset = MyDataset(train_input_data, train_target_data)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "B5AO4ziaPSeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = [(i, j) for i, j in zip(train_input_data[:10], train_target_data[:10])]\n",
        "batch\n",
        "collate_fn(batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otCElXNeYtfI",
        "outputId": "69567521-9cec-40c2-dfaf-6c0351297e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 1, 66, 58, 34, 38, 32, 63, 59, 62, 23,  7, 39, 38, 34,  2,  0,  0],\n",
              "         [ 1, 51, 64, 37, 56, 52, 48, 38, 59, 38, 20, 26, 39, 64, 37, 39,  2],\n",
              "         [ 1, 51, 38, 32, 46,  7, 21, 38,  8, 55, 38, 21,  7, 20, 63, 14,  2],\n",
              "         [ 1, 51, 38, 32, 46,  7, 20, 54,  5, 38, 46,  7,  2,  0,  0,  0,  0],\n",
              "         [ 1, 37,  5, 38, 39,  7, 39, 34,  9, 14, 39,  2,  0,  0,  0,  0,  0],\n",
              "         [ 1, 22, 41,  7, 59, 38, 66, 55,  7, 32, 38, 66, 19, 46, 19,  2,  0],\n",
              "         [ 1, 17, 21, 38, 32, 38, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 1, 59, 52, 41,  9, 13,  9,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 1, 37, 52, 66,  7, 55,  7, 39, 38, 34, 38, 32,  9,  6,  2,  0,  0],\n",
              "         [ 1,  6,  7,  8, 19, 59, 19, 51, 54, 32,  8, 19,  2,  0,  0,  0,  0]]),\n",
              " tensor([[ 1, 21,  3, 18, 12, 17, 15,  7, 15, 23, 18, 22, 12,  2,  0,  0],\n",
              "         [ 1, 24, 20, 25,  6, 22,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 1, 24, 17,  3, 12, 18, 16, 25,  3, 19, 16, 18, 14, 15, 19,  2],\n",
              "         [ 1, 24, 17,  3, 12, 18, 14, 27,  5, 12, 18,  2,  0,  0,  0,  0],\n",
              "         [ 1, 13,  5, 22, 18, 22,  3, 12, 13, 19, 22,  2,  0,  0,  0,  0],\n",
              "         [ 1,  8, 18,  7, 21, 19, 18, 17, 21,  3, 12,  3,  2,  0,  0,  0],\n",
              "         [ 1,  3, 16, 17, 23, 20,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 1,  7,  3, 16, 20, 13, 23, 13,  2,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 1,  3, 16, 21, 18, 19, 18, 22, 12, 17, 13, 22, 20,  2,  0,  0],\n",
              "         [ 1, 22, 20, 18, 25,  3,  7,  3, 24, 27, 17, 25,  3,  2,  0,  0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer, print_interval=10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    # train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for step, (src, tgt) in enumerate(train_dataloader,1):\n",
        "        src = src.transpose(0,1)\n",
        "        tgt = tgt.transpose(0,1)\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if step % print_interval == 0:\n",
        "            avg_loss = total_loss / step\n",
        "            print(f\"Step [{step}/{len(train_dataloader)}], Train loss: {avg_loss:.3f}\")\n",
        "\n",
        "    return total_loss / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "# def evaluate(model):\n",
        "#     model.eval()\n",
        "#     losses = 0\n",
        "\n",
        "#     val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "#     val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "#     for src, tgt in val_dataloader:\n",
        "#         src = src.to(DEVICE)\n",
        "#         tgt = tgt.to(DEVICE)\n",
        "\n",
        "#         tgt_input = tgt[:-1, :]\n",
        "\n",
        "#         src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "#         logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "#         tgt_out = tgt[1:, :]\n",
        "#         loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "#         losses += loss.item()\n",
        "\n",
        "#     return losses / len(list(val_dataloader))\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    # val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "mofBVWaMRFim",
        "outputId": "93911239-4362-4a4e-c64e-f3bf0ec8b3e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [10/40], Train loss: 0.971\n",
            "Step [20/40], Train loss: 0.982\n",
            "Step [30/40], Train loss: 0.989\n",
            "Step [40/40], Train loss: 1.023\n",
            "Epoch: 1, Train loss: 1.023, Val loss: 1.023, Epoch time = 9.963s\n",
            "Step [10/40], Train loss: 1.010\n",
            "Step [20/40], Train loss: 0.999\n",
            "Step [30/40], Train loss: 0.995\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-45f893d4ec51>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# val_loss = evaluate(transformer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-45f893d4ec51>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, print_interval)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-313769fba6b0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0msrc_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_tok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtgt_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_tok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n\u001b[0m\u001b[1;32m     73\u001b[0m                                 src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the feature number of src and tgt must be equal to d_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[1;32m    147\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;31m# feed forward block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = torch.tensor(encode_input(\"@\"+src_sentence+\"$\")).reshape(-1,1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \"\".join(decode_output(list(tgt_tokens.cpu().numpy()))).replace(\"@\", \"\").replace(\"$\", \"\")\n",
        "\n",
        "samples = [\"ड्रिल\",\"अन्त्यन्त\",\"उलझानों\",\"प्रतिष्ठावाले\"]\n",
        "idx = torch.randint(0,len(samples),(1,)).item()\n",
        "print(samples[idx])\n",
        "print(translate(transformer, samples[idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYNEK9KPfp4z",
        "outputId": "c6180dc3-2d3f-4c4c-e516-7cb9fdda5b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "प्रतिष्ठावाले\n",
            "pratishale\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJTMK6KQi3-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f2lwhP8ti4A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGEv8Irqi4C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXgHipD3i4FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qOzEj_sIi4HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X8mNDDKti4Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch import Tensor\n",
        "# from torch.nn import Transformer\n",
        "# import math\n",
        "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "# class PositionalEncoding(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                  emb_size: int,\n",
        "#                  dropout: float,\n",
        "#                  maxlen: int = 5000):\n",
        "#         super(PositionalEncoding, self).__init__()\n",
        "#         den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "#         pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "#         pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "#         pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "#         pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "#         pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "#     def forward(self, token_embedding: Tensor):\n",
        "#         return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# # helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "# class TokenEmbedding(nn.Module):\n",
        "#     def __init__(self, vocab_size: int, emb_size):\n",
        "#         super(TokenEmbedding, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "#         self.emb_size = emb_size\n",
        "\n",
        "#     def forward(self, tokens: Tensor):\n",
        "#         return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# class Seq2SeqTransformer(nn.Module):\n",
        "\n",
        "#     def __init__(self, input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, max_seq_length, src_pad_token_id, tgt_pad_token_id):\n",
        "#         super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "#         self.embedding_src = nn.Embedding(input_vocab_size, d_model, padding_idx = src_pad_token_id)\n",
        "#         self.embedding_tgt = nn.Embedding(output_vocab_size, d_model)\n",
        "#         self.transformer = nn.Transformer(\n",
        "#             d_model=d_model,\n",
        "#             nhead=nhead,\n",
        "#             num_encoder_layers=num_encoder_layers,\n",
        "#             num_decoder_layers=num_decoder_layers\n",
        "#         )\n",
        "#         self.fc = nn.Linear(d_model, output_vocab_size)\n",
        "#         self.max_seq_length = max_seq_length\n",
        "#         self.src_pad_token_id = src_pad_token_id\n",
        "#         self.tgt_pad_token_id = tgt_pad_token_id\n",
        "\n",
        "#     def forward(self, src, tgt):\n",
        "#         src_padding_mask = self.generate_padding_mask(src, self.src_pad_token_id)\n",
        "#         tgt_padding_mask = self.generate_padding_mask(tgt, self.tgt_pad_token_id)\n",
        "\n",
        "#         src_embedding = self.embedding_src(src)\n",
        "#         tgt_embedding = self.embedding_tgt(tgt)\n",
        "\n",
        "#         # Apply the transformer model\n",
        "#         # output = self.transformer(src_embedding.transpose(0, 1), tgt_embedding.transpose(0, 1))\n",
        "#         # output = self.transformer(src_embedding.transpose(0, 1), tgt_embedding.transpose(0, 1),\n",
        "#         #                           src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "#         output = self.transformer(src_embedding.transpose(0, 1), tgt_embedding.transpose(0, 1),\n",
        "#                                 src_mask=None, tgt_mask=None,\n",
        "#                                 src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask)\n",
        "\n",
        "#         # Convert the output to logits\n",
        "#         output_logits = self.fc(output)\n",
        "\n",
        "#         return output_logits\n",
        "\n",
        "#     # def generate_padding_mask(self, src):\n",
        "#     #     # return (src == self.src_pad_token_id).transpose(0, 1)\n",
        "#     #       return (src == self.src_pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "#     def generate_padding_mask(self, seq, pad_token_id):\n",
        "#         return (seq == pad_token_id)\n",
        "\n",
        "#     # def generate_square_subsequent_mask(self, sz):\n",
        "#     #     mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "#     #     mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "#     #     return mask\n",
        "\n",
        "# # Example parameters\n",
        "# input_vocab_size = len(stoi_input)  # Size of the input vocabulary\n",
        "# output_vocab_size = len(stoi_output)  # Size of the output vocabulary\n",
        "# max_seq_length = 50       # Maximum sequence length\n",
        "# d_model = 128             # Dimension of the model\n",
        "# nhead = 4                 # Number of attention heads\n",
        "# num_encoder_layers = 1    # Number of encoder layers\n",
        "# num_decoder_layers = 1    # Number of decoder layers\n",
        "# src_pad_token_id = input_pad_idx\n",
        "# tgt_pad_token_id = output_pad_idx\n",
        "# num_epochs = 1\n",
        "\n",
        "# # Create the model\n",
        "# model = Seq2SeqTransformer(input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, max_seq_length, src_pad_token_id, tgt_pad_token_id)\n",
        "\n",
        "# # Sample input and output sequences\n",
        "# src_sequence = torch.randint(0, input_vocab_size, (10, max_seq_length))  # Batch size of 10\n",
        "# tgt_sequence = torch.randint(0, output_vocab_size, (10, max_seq_length + 8))  # Batch size of 10, output sequence longer than input sequence\n",
        "\n",
        "# print(src_sequence.shape, tgt_sequence.shape)\n",
        "\n",
        "# # Forward pass\n",
        "# output_logits = model(src_sequence, tgt_sequence)\n",
        "# print(\"Output Logits Shape:\", output_logits.shape)\n",
        "\n",
        "# # Define your optimizer and loss function\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # Training loop (simplified example)\n",
        "# for epoch in range(num_epochs):\n",
        "#     for i, batch in enumerate(train_dataloader):  # Iterate over your data batches\n",
        "#         src_batch, tgt_batch = batch\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Forward pass\n",
        "#         output = model(src_batch, tgt_batch[:, :-1]) # model(src_batch, tgt_batch[:, :-1])  # Exclude the last token from target\n",
        "\n",
        "#         # Calculate loss\n",
        "#         # print(output.shape, tgt_batch.shape)\n",
        "#         # print(output.reshape(-1, output_vocab_size).shape)\n",
        "#         # print(tgt_batch[:, 1:].reshape(-1).shape)\n",
        "#         loss = criterion(output.reshape(-1, output_vocab_size), tgt_batch[:, 1:].reshape(-1)) # criterion(output.reshape(-1, output_vocab_size), tgt_batch[:, :].reshape(-1))  # Exclude the first token from target\n",
        "#         print(f\"step: {i}, loss: {loss.item():.2f}\")\n",
        "#         # Backpropagation and optimization\n",
        "#         loss.backward()\n",
        "#         optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UebzS1F2kZSE",
        "outputId": "51cd3855-d0c0-43d9-b393-6632c0b5f384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 50]) torch.Size([10, 58])\n",
            "Output Logits Shape: torch.Size([58, 10, 29])\n",
            "step: 0, loss: 3.62\n",
            "step: 1, loss: 2.03\n",
            "step: 2, loss: 2.12\n",
            "step: 3, loss: 2.11\n",
            "step: 4, loss: 2.16\n",
            "step: 5, loss: 2.12\n",
            "step: 6, loss: 1.96\n",
            "step: 7, loss: 1.93\n",
            "step: 8, loss: 1.78\n",
            "step: 9, loss: 1.75\n",
            "step: 10, loss: 1.91\n",
            "step: 11, loss: 2.08\n",
            "step: 12, loss: 1.99\n",
            "step: 13, loss: 2.12\n",
            "step: 14, loss: 2.13\n",
            "step: 15, loss: 1.83\n",
            "step: 16, loss: 2.14\n",
            "step: 17, loss: 2.18\n",
            "step: 18, loss: 1.81\n",
            "step: 19, loss: 2.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logits = model(src_sequence, tgt_sequence[:, :-1])\n",
        "# print(logits.reshape(-1, output_vocab_size).shape)\n",
        "# print(tgt_sequence[:, 1:].reshape(-1).shape)\n",
        "# # print(logits.reshape(-1, output_vocab_size), tgt_batch[:, 1:].reshape(-1))\n",
        "# # criterion(logits, tgt_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4YBOY2lBYXa",
        "outputId": "b50c4c95-d26b-46c3-c70a-c9df3d84d46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([570, 27])\n",
            "torch.Size([570])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# def transliterate_word(model, src, max_length=50):\n",
        "#     model.eval()\n",
        "\n",
        "#     # Tokenize the source sentence and convert to tensor\n",
        "#     src_tokens = encode_input(src)\n",
        "#     src_tensor = torch.tensor(src_tokens).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "#     # Initialize the target sentence with the <sos> token\n",
        "#     tgt = ['@']\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for _ in range(max_length):\n",
        "#             # Convert the current target sentence to tensor\n",
        "#             tgt_tokens = encode_output(tgt)\n",
        "#             # print(tgt_tokens)\n",
        "#             tgt_tensor = torch.tensor(tgt_tokens).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "#             # Generate the next token in the target sequence\n",
        "#             output = model(src_tensor, tgt_tensor)\n",
        "#             # print(output)\n",
        "\n",
        "#             # next_token_id = output.argmax(dim=-1)[:, -1].item()\n",
        "\n",
        "#             next_token_probs = output[0, -1, :]  # Get the probabilities for the next token\n",
        "#             print(next_token_probs)\n",
        "#             next_token_id = next_token_probs.argmax().item()\n",
        "\n",
        "#             next_token = decode_output([next_token_id])\n",
        "#             print(next_token)\n",
        "\n",
        "#             # Stop if <eos> token is generated or if max length is reached\n",
        "#             if next_token == '$' or len(tgt) >= max_length:\n",
        "#                 break\n",
        "\n",
        "#             tgt.append(next_token)\n",
        "\n",
        "#     # Convert the target sentence back to a string\n",
        "#     transliterated_word = ' '.join(tgt[1:])  # Exclude <sos> token\n",
        "#     return transliterated_word\n",
        "\n",
        "# # Assuming you have trained 'model', 'src_vocab', and 'tgt_vocab'\n",
        "\n",
        "# sample = \"ड्रिल\"\n",
        "# # sample = \"उलझानों\"\n",
        "# output = transliterate_word(model, sample)\n",
        "# print(\"Source Sentence:\", sample)\n",
        "# print(\"Translated Sentence:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "bL2RZQJuqvUM",
        "outputId": "036f5cf1-830b-40bc-8dc1-762302bc509d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-60f7dab1be79>\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ड्रिल\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# sample = \"उलझानों\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransliterate_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Source Sentence:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Translated Sentence:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a = torch.tensor([[[-0.0194,  2.4957,  1.2230, -2.5968, -1.3113, -2.7148,  0.9393,\n",
        "#            1.2988, -0.5863,  1.1398,  0.6810,  0.1490, -1.8229,  1.2060,\n",
        "#           -0.3787, -2.8461,  0.0497, -0.4994, -0.5241,  0.9051,  0.9477,\n",
        "#            1.6556,  0.4972,  0.7228,  0.7183, -0.1150,  4.9022]],\n",
        "\n",
        "#         [[-0.0225,  2.4873,  1.2296, -2.5983, -1.3164, -2.7160,  0.9366,\n",
        "#            1.2947, -0.5868,  1.1381,  0.6856,  0.1497, -1.8209,  1.2084,\n",
        "#           -0.3769, -2.8440,  0.0500, -0.5049, -0.5246,  0.9100,  0.9507,\n",
        "#            1.6576,  0.4980,  0.7252,  0.7146, -0.1176,  4.8899]]])\n",
        "# a.argmax(dim=-1)[:, -1]#.item()\n",
        "\n",
        "torch.tensor([1,2]).unsqueeze(0)  # Add batch dimension\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBCDEBfP9NW7",
        "outputId": "64e28897-c5fc-412e-b808-e7f0caab730e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGfpWqNWouZz",
        "outputId": "049f8dfd-b5df-45a8-95b0-3e02132e3021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqTransformer(\n",
              "  (embedding_src): Embedding(70, 64, padding_idx=64)\n",
              "  (embedding_tgt): Embedding(27, 64)\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
              "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
              "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=64, out_features=27, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NpjOTWztoub0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HY_Kt5-4oudw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4TFclCCnouhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# for batch_input, batch_target in train_dataloader:\n",
        "#   print(batch_input)\n",
        "#   print(batch_target)\n",
        "#   break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgKsERacc9EK",
        "outputId": "c618d805-19eb-4e54-fbd8-abb3d09a68c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[14, 55, 17, 24, 20, 55, 24, 20, 47, 48],\n",
            "        [47, 34,  3, 48, 23, 48, 63, 63, 63, 63],\n",
            "        [60, 49, 52, 27, 21, 63, 63, 63, 63, 63],\n",
            "        [62, 17, 55, 11, 18, 20, 47, 27, 63, 63]])\n",
            "(tensor([10, 25, 23, 20, 16, 25, 25, 23, 18, 25, 25, 14, 13]), tensor([14, 25,  7, 24, 13, 15, 13]), tensor([ 8, 24,  1,  1,  3, 20, 13]), tensor([ 9, 20, 23, 22, 25, 25, 14, 20]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# g = torch.Generator()\n",
        "# g.manual_seed(123)\n",
        "# for _ in range(5):\n",
        "#   print(torch.randint(0,len(x), (2,), generator=g).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD0PlmTqFS4B",
        "outputId": "0f7ecd34-8922-4c59-801d-afcefecb732e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[382, 789]\n",
            "[102, 610]\n",
            "[580, 842]\n",
            "[886, 57]\n",
            "[699, 754]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# g = torch.Generator()\n",
        "# g.manual_seed(123)\n",
        "# def get_batch(batch_size):\n",
        "#   idx = torch.randint(0,len(x), (2,), generator=g).tolist()\n",
        "#   return x[idx]\n",
        "# get_batch(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "7SNiqbogGhJ2",
        "outputId": "b8eb66be-cf24-4391-f643-914ab11a999f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-c3e69ea13937>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-c3e69ea13937>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import math\n",
        "\n",
        "\n",
        "# class PositionalEncoding(nn.Module):\n",
        "#     def __init__(self, d_model, max_len=1000):\n",
        "#         super(PositionalEncoding, self).__init__()\n",
        "#         self.dropout = nn.Dropout(p=0.1)\n",
        "#         pe = torch.zeros(max_len, d_model)\n",
        "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "#         self.register_buffer('pe', pe)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.pe[:x.size(0), :]\n",
        "#         return self.dropout(x)\n",
        "\n",
        "# class TransformerTextToText(nn.Module):\n",
        "#     def __init__(self, input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
        "#         super(TransformerTextToText, self).__init__()\n",
        "#         self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "#         self.positional_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "#         self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "#         self.decoder_embedding = nn.Embedding(output_vocab_size, d_model)\n",
        "#         self.decoder_positional_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "#         self.fc = nn.Linear(d_model, output_vocab_size)\n",
        "\n",
        "#     def forward(self, src, tgt):\n",
        "#         src = self.embedding(src)\n",
        "#         src = self.positional_encoding(src)\n",
        "\n",
        "#         tgt = self.decoder_embedding(tgt)\n",
        "#         tgt = self.decoder_positional_encoding(tgt)\n",
        "\n",
        "#         output = self.transformer(src, tgt)\n",
        "#         output = self.fc(output)\n",
        "\n",
        "#         return output\n",
        "\n",
        "# # Define hyperparameters\n",
        "# input_vocab_size = len(hin_vocab) + len(eng_vocab)\n",
        "# output_vocab_size = len(eng_vocab)\n",
        "# d_model = 32\n",
        "# nhead = 4\n",
        "# num_encoder_layers = 1\n",
        "# num_decoder_layers = 1\n",
        "# model = TransformerTextToText(input_vocab_size, output_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training loop\n",
        "# def train(input_tensor, target_tensor):\n",
        "#     optimizer.zero_grad()\n",
        "#     output = model(input_tensor, target_tensor)\n",
        "#     output = output.view(-1, output_vocab_size)\n",
        "#     target_tensor = target_tensor.view(-1)\n",
        "#     loss = criterion(output, target_tensor)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n"
      ],
      "metadata": {
        "id": "rEBBRecbAND-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# train()"
      ],
      "metadata": {
        "id": "TsftVYdYClgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage\n",
        "# input_hindi_tensor = encode_sentence(\"भारत\", hindi_char_to_index)\n",
        "# input_english_tensor = encode_sentence(\"bharat\", english_char_to_index)\n",
        "# input_tensor = torch.cat((input_hindi_tensor, input_english_tensor), dim=0).unsqueeze(1)\n",
        "\n",
        "# target_english_tensor = encode_sentence(\"india\", english_char_to_index)  # Only English output\n",
        "# target_tensor = target_english_tensor.unsqueeze(1)\n",
        "\n",
        "# loss = train(input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "DhMx9livCe-G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}